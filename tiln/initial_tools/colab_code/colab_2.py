# -*- coding: utf-8 -*-
"""Tiln_proiect_offensive_detections.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CGRRbnVj3YQlHKQk81loqkCWHz2kCmbg
"""

#!pip install stop-words
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import json
import requests
import re
#import pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import svm

from nltk.corpus import stopwords
from stop_words import get_stop_words
from nltk.tokenize import word_tokenize
from string import punctuation

#Functii :
def lematizare_si_pos_tag_racai(comment):
	
	#Daca trimitem comment-ul ca o lista il refacem in comment cu spatiu intre cuvinte pentru a nu face prea multe request-uri la API
	#TO DO?:Probabil chiar sa trimitem mai multe comment-uri odata
	if isinstance(comment, list):
		comment_string = ""
		for cuvant in comment: 
			comment_string = comment_string + " " + cuvant
		comment = comment_string
		
	racai_url = 'http://relate.racai.ro:5000/process'
	comment_data = {'text': comment,
					'exec': 'lemmatization'}

	racai_answer = requests.post(racai_url, data = comment_data)
	racai_answer_json = json.loads(racai_answer.text)
	lemmas = []
	# Procesare answer :
	racai_answer_json_tokens = racai_answer_json["teprolin-result"]["tokenized"][0]
	for token in racai_answer_json_tokens:
		# print(token["_lemma"], token["_ctg"], token["_deprel"], token["_msd"])
		lemmas.append((token["_lemma"],token["_ctg"]))
	
	return lemmas
	


def read_corpus_file(corpus_file_path):
    """ Functie de citire csv corpus si spargere in vectori de categorii cu toate commenturile din aceiasi categorie""" 
    """ Csv de forma asta pe linie: "categorie,comment" """
    """ categorie=0 ->offensive categorie=1 ->neutru categorie=2 ->nonoffensive """
    
    corpus = open(corpus_file_path, 'r', encoding='utf-8')

    linie_gresita = 0
    
    corpus_offensive = []
    corpus_nonoffensive = []
    corpus_neutru = []
    
    for linie in corpus:
        #scoatem enter
        linie = linie.strip()
        #spargem doar dupa prima virgula
        linie_sparta = linie.split(",", 1) 
        if len(linie_sparta) == 2:
            categorie , comment = linie_sparta
            if (categorie == "1"):
                corpus_offensive.append(comment)
            # elif (categorie == "2"):
            #     corpus_neutru.append(comment)
            elif (categorie == "0"):
                corpus_nonoffensive.append(comment)
            else:
                #print("Linie gresita tip1 : categorie=" , categorie , "comment=",comment)
                linie_gresita = linie_gresita + 1
        else:
            #print("Linie gresita tip2:" , linie)
            linie_gresita = linie_gresita + 1
        total = len(corpus_offensive) + len(corpus_nonoffensive) + len(corpus_neutru) + linie_gresita
        
    print("Avem" , total , " linii citite corect si " , linie_gresita , " incorecte in fisier ")
    #print(" Offensive =" , len(corpus_offensive) , "\n Nonoffensive =" , len(corpus_nonoffensive) , "\n Neutru =" , len(corpus_neutru))
    print(" Offensive =" , len(corpus_offensive) , "\n Nonoffensive =" , len(corpus_nonoffensive))
    return corpus_offensive , corpus_nonoffensive , corpus_neutru



def procesare_comments(initial_comments):
  comments_processed = []
  for num, comment in enumerate( initial_comments, start=1):
    #if (num <= 10):# lucrez temporar doar pe o parte din comment-uri
    # examplu_comment_ro = """Aceasta este un comment, pe care incerc sa il tokenizez, sa ii scot stopwords , semnele de punctuatie si sa il lematizez."""
    comment_intial = comment
    #1.1.0 lowercase si alte cazuri speciale de indepartat
    comment = comment.lower()
    comment = re.sub(r'[^\w\s]',' ',comment, re.UNICODE)
    comment = re.sub(r'[\s\d*\s]',' ',comment, re.UNICODE)
    
    #1.1.1 tokenizare
    comment_tokenizat = word_tokenize(comment)  
    # print("Comment ro tokenizat:\n",comment_tokenizat)
    
    #1.1.2 stergem stopwords si cuvintele de 1 caracter
    stop_words_ro = set(stopwords.words('romanian'))
    #comment_fara_stopwords = [w for w in comment_tokenizat if ((not w in stop_words_ro) and  len(w) >= 2) ]  
    comment_fara_stopwords = [w for w in comment_tokenizat if (not w in stop_words_ro)] 

    # print("Comment ro tokenizat fara stopwords:\n",comment_fara_stopwords)
    
    #1.1.3 stergem punctuatia 
    comment_fara_puncte = [w for w in comment_fara_stopwords if (not w in punctuation)]
    # print("Comment ro tokenizat fara stopwords si puncte:\n",comment_fara_puncte)

    #1.1.4 transformare in lema si pos tag -> dureaza prea mult ->trebuie optimizata cumva
    toate_cuvintele_comment = []
    """
    comment_lemmizat_si_postag = lematizare_si_pos_tag_racai(comment_fara_puncte)#pentru fiecare comment extragem lema si pos tag-ul pe baza API racai
    # print("Comment ro tokenizat fara stopwords si puncte + lematizare si pos tagging:\n",comment_lemmizat_si_postag)
    for no_cuv ,tupla_lema_tag in enumerate(comment_lemmizat_si_postag, start=0):
      lema = tupla_lema_tag[0]
      tag = tupla_lema_tag[1]
      # ('urechi', 'NOUN')
      # ('vorbi', 'VERB')
      # ('bÄƒiat', 'NOUN')
      # ('alin', 'ADV')
      # ('tu', 'PRON')
      # ('avea', 'AUX')
      if(re.search(r"\s|\d",lema) or len(lema) <=1 ):
        print("\n\t\t!!! Posibila erroare prepocesare ce ar trebui indepartata in lema:", lema,tag,comment_fara_puncte[no_cuv] , comment_intial)
      toate_cuvintele_comment.append(lema)#deocamdata nu folosim pos tag-ul
    """
    #Reconstruim commentul din cuvintele procesate separate de spatii
    comment_processed = ""
    for cuvant in comment_fara_puncte:
      comment_processed = comment_processed + " " + cuvant
    comments_processed.append(comment_processed)

  return comments_processed

def print_results(model_name, predicted_labels, initial_labels):
  tn, fp, fn, tp = confusion_matrix(predicted_labels, initial_labels).ravel()
  print(model_name + " Offensive - TN:", tn, "FN:", fn, "Precision:",round((tn/(tn+fn))*100,3),"%", "Recall:",round((tn/(tn+fp))*100,3),"%")
  print(model_name + " NonOffensive - TP:", tp, "FP:", fp, "Precision:",round((tp/(tp+fp))*100,3),"%", "Recall:",round((tp/(tp+fn))*100,3),"%")

#MAIN: 
#0. citire comments:
offensive , nonoffensive , neutru = read_corpus_file('corpus_final.csv')#var 1
#offensive , nonoffensive , neutru = read_corpus_file('/content/copus_lemizat_var1.csv') #var 2
#offensive , nonoffensive , neutru = read_corpus_file('/content/copus_lemizat_cu_posstag_var1.csv') #var 3
#print("Corpus offensinve" , offensive)
#print("Corpus nonoffensive" , nonoffensive)
#print("Corpus neutru" , neutru)


#1 procesare comments and add labels:
all_comments = np.concatenate((offensive, nonoffensive))
#all_comments = np.concatenate((all_comments, neutru))
all_lables = [1 ] * len(offensive) + [0] * len(nonoffensive)
print(len(all_comments),len(all_lables))
all_comments_processed = procesare_comments(all_comments)
stop_words_ro = get_stop_words('ro')



# Impartire comments/labels in date antrenare si date test
train_comments, test_comments, train_labels, test_labels = train_test_split( all_comments_processed, all_lables, test_size=0.2, random_state = 0)


comments_word_conts = CountVectorizer(stop_words=stop_words_ro)
train_data = comments_word_conts.fit_transform(train_comments)
tfidf_transformer  = TfidfTransformer(sublinear_tf=True)
train_tfidf = tfidf_transformer.fit_transform(train_data)

test_data = comments_word_conts.transform(test_comments)
test_tfidf = tfidf_transformer.transform(test_data)


#2 antrenare Naive Bayesian
multi_naive_bayes  = MultinomialNB()
multi_naive_bayes.fit(train_tfidf, train_labels)

#3 testare Naive Bayesian
test_predicted_nb = multi_naive_bayes.predict(test_tfidf)

#4 Afisare rezultate separate pe categorii(False/True positiv, False/True negativ)
#confusion matrix
print_results("Naive Bayes", test_predicted_nb, test_labels)


#2 antrenare SVM
svm = svm.SVC(C=1000)
svm.fit(train_tfidf, train_labels)

#3 testare SVM
test_predicted_svm = svm.predict(test_tfidf)

#4 Afisare rezultate separate pe categorii(False/True positiv, False/True negativ)
print_results("SVM", test_predicted_svm, test_labels)

#2 antrenare Passive Aggressive
pa = PassiveAggressiveClassifier(C = 0.5, random_state = 5)
pa.fit(train_tfidf, train_labels)

#3 testare Passive Aggressive
test_predicted_pa = pa.predict(test_tfidf)

#4 Afisare rezultate separate pe categorii(False/True positiv, False/True negativ)
print_results("Passive Aggressive", test_predicted_pa, test_labels)


#2 antrenare Logistic Regression
lr = LogisticRegression(solver='liblinear', random_state=0)
lr.fit(train_tfidf, train_labels)

#3 testare Passive Aggressive
test_predicted_lr = lr.predict(test_tfidf)

#4 Afisare rezultate separate pe categorii(False/True positiv, False/True negativ)
print_results("Logistic Regression", test_predicted_lr, test_labels)


if 0:
  #5 Si un text cu comment-uri random pe modelul antrenat si testat anterior
  comments_test = ["Loredana a ramas panarama" , "Asta e o porcarie de emisiune", "Ce imi place emisiunea", "Esti un prost" , "Esti destept" ]
  comments_test2 = procesare_comments(comments_test)
  test_conts = comments_word_conts.transform(comments_test2)
  test_tfidf = tfidf_transformer.transform(test_conts)
  test_predicted_nb = multi_naive_bayes.predict(test_tfidf)
  test_predicted_svm = svm.predict(test_tfidf)
  for comment, predicted_label_nb, predicted_label_svm in zip(comments_test, test_predicted_nb ,test_predicted_svm):
    print("Ex input comment: \"" + comment + "\" - predicted label NB output:", predicted_label_nb,  "SVM output:", predicted_label_svm)


# When a search engine returns 30 pages, only 20 of which are relevant, 
# while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3,
# which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.

#Rezultate:

# varianta 1: tfidf fara stopword si fara punctuatie
#Naive Bayes Offensive - TN: 1156 FN: 442 Precision: 72.34 % Recall: 93.907 %
#Naive Bayes NonOffensive - TP: 2120 FP: 75 Precision: 96.583 % Recall: 82.748 %
#SVM Offensive - TN: 1434 FN: 164 Precision: 89.737 % Recall: 88.738 %
#SVM NonOffensive - TP: 2013 FP: 182 Precision: 91.708 % Recall: 92.467 %
#PassiveAggressive Offensive - TN: 1407 FN: 191 Precision: 88.048 % Recall: 85.845 %
#PassiveAggressive NonOffensive - TP: 1963 FP: 232 Precision: 89.431 % Recall: 91.133 %
#Logistic Regression Offensive - TN: 1408 FN: 190 Precision: 88.11 % Recall: 87.835 %
#Logistic Regression NonOffensive - TP: 2000 FP: 195 Precision: 91.116 % Recall: 91.324 %

# varianta 2: tfidf fara stopword si fara punctuatie . cu lematizare
#Naive Bayes Offensive - TN: 1107 FN: 458 Precision: 70.735 % Recall: 93.103 %
#Naive Bayes NonOffensive - TP: 2146 FP: 82 Precision: 96.32 % Recall: 82.412 %
#SVM Offensive - TN: 1381 FN: 184 Precision: 88.243 % Recall: 87.129 %
#SVM NonOffensive - TP: 2024 FP: 204 Precision: 90.844 % Recall: 91.667 %
#Passive Aggressive Offensive - TN: 1359 FN: 206 Precision: 86.837 % Recall: 84.462 %
#Passive Aggressive NonOffensive - TP: 1978 FP: 250 Precision: 88.779 % Recall: 90.568 %
#Logistic Regression Offensive - TN: 1355 FN: 210 Precision: 86.581 % Recall: 86.416 %
#Logistic Regression NonOffensive - TP: 2015 FP: 213 Precision: 90.44 % Recall: 90.562 % 

# varianta 3: tfidf fara stopword si fara punctuatie . cu lematizare si postag
#Naive Bayes Offensive - TN: 1028 FN: 537 Precision: 65.687 % Recall: 93.54 %
#Naive Bayes NonOffensive - TP: 2157 FP: 71 Precision: 96.813 % Recall: 80.067 %
#SVM Offensive - TN: 1367 FN: 198 Precision: 87.348 % Recall: 86.137 %
#SVM NonOffensive - TP: 2008 FP: 220 Precision: 90.126 % Recall: 91.024 %
#Passive Aggressive Offensive - TN: 1353 FN: 212 Precision: 86.454 % Recall: 83.108 %
#Passive Aggressive NonOffensive - TP: 1953 FP: 275 Precision: 87.657 % Recall: 90.208 %
#Logistic Regression Offensive - TN: 1315 FN: 250 Precision: 84.026 % Recall: 86.286 %
#Logistic Regression NonOffensive - TP: 2019 FP: 209 Precision: 90.619 % Recall: 88.982 %
